{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f53b449",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565592d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import lyricsgenius as lg\n",
    "\n",
    "#Token\n",
    "genius = lg.Genius('YOUR TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c5cf0",
   "metadata": {},
   "source": [
    "## 2. Descargar Albumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d03a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "album = genius.search_album(\"Un Verano Sin Ti\", \"Bad Bunny\")\n",
    "album.save_lyrics()\n",
    "\n",
    "YHLQMDLG = genius.search_album(\"YHLQMDLG\", \"Bad Bunny\")\n",
    "YHLQMDLG.save_lyrics()\n",
    "\n",
    "Ultimo_Tour = genius.search_album(\"EL ÚLTIMO TOUR DEL MUNDO\", \"Bad Bunny\")\n",
    "Ultimo_Tour.save_lyrics()\n",
    "\n",
    "LQNIAS = genius.search_album(\"LAS QUE NO IBAN A SALIR\", \"Bad Bunny\")\n",
    "LQNIAS.save_lyrics()\n",
    "\n",
    "X100 = genius.search_album(\"x 100PRE\", \"Bad Bunny\")\n",
    "X100.save_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bf0dc",
   "metadata": {},
   "source": [
    "## 3. Carga de datos y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef32c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos en formato json\n",
    "\n",
    "Un_Verano_json = json.load(open('Lyrics_UnVeranoSinTi.json'))\n",
    "YHLQMDLG_json = json.load(open('Lyrics_YHLQMDLG.json'))\n",
    "Ultimo_Tour_json = json.load(open('Lyrics_ELÚLTIMOTOURDELMUNDO.json'))\n",
    "LQNIAS_json = json.load(open('Lyrics_LASQUENOIBANASALIR.json'))\n",
    "X100_json = json.load(open('Lyrics_X100PRE.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1414c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos dataframes vacíos para los datos del json file\n",
    "\n",
    "Un_Verano_df = pd.DataFrame(columns=['Song', 'Artist','Album', 'Lyrics'])\n",
    "YHLQMDLG_df = pd.DataFrame(columns=['Song', 'Artist','Album', 'Lyrics'])\n",
    "Ultimo_Tour_df = pd.DataFrame(columns=['Song', 'Artist','Album', 'Lyrics'])\n",
    "LQNIAS_df = pd.DataFrame(columns=['Song', 'Artist','Album', 'Lyrics'])\n",
    "X100_df = pd.DataFrame(columns=['Song', 'Artist','Album', 'Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d1b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una función que itere sobre los elementos del json y los añada al df.\n",
    "\n",
    "def jsontodf (data, df):\n",
    "    time.sleep(1)\n",
    "    for i in data['tracks']:\n",
    "        song = i['song']['title']\n",
    "        artist = i['song']['artist']\n",
    "        lyrics = i['song']['lyrics']\n",
    "              \n",
    "        df = df.append({'Song': song, 'Artist':artist, 'Lyrics':lyrics}, ignore_index=True)\n",
    "    \n",
    "    #Llenamos el album con una condición: si x canción es la primera, entonces el album es y.\n",
    "\n",
    "    if df.Song.iloc[0] == 'Moscow Mule':\n",
    "        df.fillna('Un Verano Sin Ti', inplace=True)\n",
    "    elif df.Song.astype(str).iloc[0] == 'Si Veo a Tu Mamá':\n",
    "        df.fillna('YHLQMDLG', inplace=True)\n",
    "    elif df.Song.astype(str).iloc[0] =='EL MUNDO ES MÍO':\n",
    "        df.fillna('EL ULTIMO TOUR DEL MUNDO', inplace=True)\n",
    "    elif df.Song.astype(str).iloc[0] =='SI ELLA SALE':\n",
    "        df.fillna('LAS QUE NO IBAN A SALIR', inplace=True)\n",
    "    elif df.Song.astype(str).iloc[0] =='NI BIEN NI MAL':\n",
    "        df.fillna('X100pre', inplace=True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b04e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos la función a los 5 albumes\n",
    "\n",
    "Un_Verano_df = jsontodf(Un_Verano_json, Un_Verano_df)\n",
    "YHLQMDLG_df = jsontodf(YHLQMDLG_json, YHLQMDLG_df)\n",
    "Ultimo_Tour_df = jsontodf(Ultimo_Tour_json, Ultimo_Tour_df)\n",
    "LQNIAS_df = jsontodf(LQNIAS_json, LQNIAS_df)\n",
    "X100_df = jsontodf(X100_json,X100_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c342cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos los elementos en Lyric (la palabra que precede a las letras), y tomamos el segundo elemento, es decir,\n",
    "#lo que está luego de la intro.\n",
    "\n",
    "def lyrics_split(df):\n",
    "    df = df.convert_dtypes(str)\n",
    "    \n",
    "    letra = df.Lyrics.apply(lambda x: x.split('Lyrics',1)[1])\n",
    "    \n",
    "    df.Lyrics = letra\n",
    "    \n",
    "    df = df.convert_dtypes(str)\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c5ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos la función a los 5 albumes\n",
    "\n",
    "Un_Verano_df = lyrics_split(Un_Verano_df)\n",
    "YHLQMDLG_df = lyrics_split(YHLQMDLG_df)\n",
    "Ultimo_Tour_df = lyrics_split(Ultimo_Tour_df)\n",
    "LQNIAS_df = lyrics_split(LQNIAS_df)\n",
    "X100_df = lyrics_split(X100_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb1e19",
   "metadata": {},
   "source": [
    "## 4. Word Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a31966fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una función que convierte las letras en lista, separa todas las palabras y hace que sean un elemento separado y\n",
    "#devuelva otra lista con esas palabras separadas.\n",
    "\n",
    "#Como las stopwords de ntlk no incluyen algunas palabras de canciones de este género, como yeh, to'... añadimos esas\n",
    "#palabras que no aportan información a las stopwords de ntlk. Además otras palabras que buscamos limpiar, como coro, estribillo...\n",
    "\n",
    "#Este código cuenta la frecuencia de cada palabra por separado, sin embargo, puerto rico se repite mucho en sus canciones y \n",
    "#lo cuenta por separado. Entonces, como siempre que dice puerto viene seguido de un rico, pero dice rico en otras oportunidades\n",
    "#(no lo precede un puerto), en la función añado 'rico' luego de puerto en el df, y resto la frecuencia de 'puerto rico' a 'rico'\n",
    "\n",
    "def word_count(data):\n",
    "    \n",
    "    lista = data.Lyrics.tolist()\n",
    "    \n",
    "    palabras = ' '\n",
    "    \n",
    "    for x in lista:\n",
    "        palabras += ' ' + x\n",
    "    \n",
    "    token=re.findall('\\w+', palabras)\n",
    "       \n",
    "    words = []\n",
    "    \n",
    "    for word in token:\n",
    "        words.append(word.lower())\n",
    "    \n",
    "    sw = nltk.corpus.stopwords.words('spanish')\n",
    "    sw_bb = ['bad', 'bunny', 'pa', 'ey', 'eh', 'verso', 'coro', 'estribillo', 'interludio'\n",
    "             , 'puente', 'pre-coro','outro','yeh','refran','letra', 'intro', 'oh','si', 'quiere', 'ah'\n",
    "            , 'yeah', 'sé', 'vo', 'va','to', 'voy', 'así', 'bien', 'quiero', '25embed', 'also'\n",
    "            ,'might', 'you', 'dar', 'tó', 'vamo', 'like', '1', 'je','ere', 'hace', '2', '3', '4','72embed', 'quién',\n",
    "            'uh','wuh','r', 'nah','puedo','pre','hago','viene','d','wouh','ser']\n",
    "    \n",
    "    words_ne=[]\n",
    "    for word in words:\n",
    "        if word not in sw:\n",
    "            if word not in sw_bb:\n",
    "                words_ne.append(word)\n",
    "    \n",
    "    nlp_words=nltk.FreqDist(words_ne)\n",
    "    nlp_words\n",
    "    \n",
    "    result = pd.DataFrame(nlp_words.items(), columns=['word', 'frequency'])\n",
    "       \n",
    "    result.loc[result.word == 'puerto', 'word'] = 'puerto rico'\n",
    "      \n",
    "    pr = result.loc[result.word == 'puerto rico'].frequency\n",
    "        \n",
    "    prr = pr.tolist()\n",
    "        \n",
    "    prrr = prr[0]\n",
    "        \n",
    "    prrrr = result.loc[result.word == 'rico'].frequency - prrr\n",
    "        \n",
    "    result.loc[result.word == 'rico', 'frequency'] = prrrr\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1ac3aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta función hace exactamente lo mismo que la anterior, solo que la usé para albumes que no dice Puerto Rico (x100pre y Ultimo Tour del Mundo)\n",
    "\n",
    "def word_counter(data):\n",
    "    \n",
    "    lista = data.Lyrics.tolist()\n",
    "    \n",
    "    palabras = ' '\n",
    "    \n",
    "    for x in lista:\n",
    "        palabras += ' ' + x\n",
    "    \n",
    "    token=re.findall('\\w+', palabras)\n",
    "       \n",
    "    words = []\n",
    "    \n",
    "    for word in token:\n",
    "        words.append(word.lower())\n",
    "    \n",
    "    sw = nltk.corpus.stopwords.words('spanish')\n",
    "    sw_bb = ['bad', 'bunny', 'pa', 'ey', 'eh', 'verso', 'coro', 'estribillo', 'interludio'\n",
    "             , 'puente', 'pre-coro','outro','yeh','refran','letra', 'intro', 'oh','si', 'quiere', 'ah'\n",
    "            , 'yeah', 'sé', 'vo', 'va','to', 'voy', 'así', 'bien', 'quiero', '25embed', 'also'\n",
    "            ,'might', 'you', 'dar', 'tó', 'vamo', 'like', '1', 'je','ere', 'hace', '2', '3', '4','72embed', 'quién'\n",
    "            ,'uh','wuh','r','nah','puedo','pre','hago','viene','d','wouh','ser']\n",
    "    \n",
    "    words_ne=[]\n",
    "    for word in words:\n",
    "        if word not in sw:\n",
    "            if word not in sw_bb:\n",
    "                words_ne.append(word)\n",
    "    \n",
    "    nlp_words=nltk.FreqDist(words_ne)\n",
    "    nlp_words\n",
    "    \n",
    "    result = pd.DataFrame(nlp_words.items(), columns=['word', 'frequency'])\n",
    "       \n",
    "    result.loc[result.word == 'puerto', 'word'] = 'puerto rico'\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5c08fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos la función a los df y obtenemos un df con la frecuencia de las palabras.\n",
    "\n",
    "Un_Verano_words = word_count(Un_Verano_df)\n",
    "YHLQMDLG_words = word_count(YHLQMDLG_df)\n",
    "Ultimo_Tour_words = word_counter(Ultimo_Tour_df)\n",
    "LQNIAS_words = word_count(LQNIAS_df)\n",
    "X100_words = word_counter(X100_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9c533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
